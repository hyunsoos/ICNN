{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EECS504_PS4_Hyunsoo_52303111.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "086eedd7201243519363e44d478afe95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14cc1ae30bea46e4a579aa06e6ecd412",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3bcc05e448974b4b81abc6699078ec01",
              "IPY_MODEL_3142f6c64af94f08a8840c9a084f5431"
            ]
          }
        },
        "14cc1ae30bea46e4a579aa06e6ecd412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3bcc05e448974b4b81abc6699078ec01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a713413def64d2f853f1c1411392c54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a260905fa68841639d4e0b6932c415d9"
          }
        },
        "3142f6c64af94f08a8840c9a084f5431": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ebc14c94060c4964979b3eca058cf19b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:06&lt;00:00, 26693205.54it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d5bfcb43b6bc4db5a80e83160559bb24"
          }
        },
        "7a713413def64d2f853f1c1411392c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a260905fa68841639d4e0b6932c415d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebc14c94060c4964979b3eca058cf19b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d5bfcb43b6bc4db5a80e83160559bb24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjJMpc4ocIBV"
      },
      "source": [
        "# EECS 504 Problem Set 4\n",
        "\n",
        "__Please provide the following information__\n",
        "(e.g. Jason Corso, jjcorso):\n",
        "\n",
        "[Your first name] [Your last name], [Your UMich uniqname]\n",
        "\n",
        "__Important__: after you download the .ipynb file, please name it as EECS504_PS4_\\<your_uniquename\\>_\\<your_umid\\>.ipynb before you submit it to canvas. Example: EECS504_PS4_adam_01101100.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdU4c6cacMbw"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We'll provide you with starter code, like this, in a Jupyter notebook for most problem sets. Please fill in the code to complete the assignment, and submit your notebook to Canvas as a .ipynb file. You can, of course, initially write your code offline in an editor like Emacs or Vim -- we'd just like the final output to be in a notebook format to make grading more consistent.\n",
        "\n",
        "Please note that *we won't run your code*. The notebook you submit should already contain all of the results we ask for. In other words, outputs should be computed *before you submit*.  Also, please do not include long, unnecessary outputs (a few print statements and visualizations are fine, but pages of debugging messages make grading difficult).\n",
        "\n",
        "You will have to make three submissions:\n",
        "\n",
        "1) Submit the written questions in a pdf to gradescope (there will be an entry for the written part).\n",
        "\n",
        "2) Turn the notebook into a pdf file and submit it to gradescope (there will be an entry for the coding part). DO NOT USE YOUR BROWSER'S PRINT FUNCTIONALITY DIRECTLY ON THE NOTEBOOK TO GET THE PDF. You will have to download the ipynb notebook, run the command: ipython nbconvert --to html EECS_504_PS4.ipynb  on your computer, open that html file in a browser and then print as PDF. Make sure that all the images and coding blocks are visible in your pdf.\n",
        "\n",
        "3) Submit the .ipynb file directly to canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Xxj41ycS6M"
      },
      "source": [
        "# Starting\n",
        "\n",
        "Run the following code to import the modules and download all the files you'll need.. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_p0jH-yb5f-",
        "outputId": "e5428407-60b9-42a0-f02c-704c978f7574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "086eedd7201243519363e44d478afe95",
            "14cc1ae30bea46e4a579aa06e6ecd412",
            "3bcc05e448974b4b81abc6699078ec01",
            "3142f6c64af94f08a8840c9a084f5431",
            "7a713413def64d2f853f1c1411392c54",
            "a260905fa68841639d4e0b6932c415d9",
            "ebc14c94060c4964979b3eca058cf19b",
            "d5bfcb43b6bc4db5a80e83160559bb24"
          ]
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torchvision.datasets import CIFAR10\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "download = not os.path.isdir('cifar-10-batches-py')\n",
        "dset_train = CIFAR10(root='.', download=download)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "086eedd7201243519363e44d478afe95",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyDGzG8tca95"
      },
      "source": [
        "# Problem 2 Multi-layer perceptron\n",
        "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
        "\n",
        "input - fully connected layer - ReLU - fully connected layer - softmax\n",
        "\n",
        "The outputs of the second fully-connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-qFO7B8cjyv"
      },
      "source": [
        "# 2.1 Layers\n",
        "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8GCN1WDccO4"
      },
      "source": [
        "def fc_forward(x, w, b):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a fully-connected layer.\n",
        "    \n",
        "    The input x has shape (N, Din) and contains a minibatch of N\n",
        "    examples, where each example x[i] has shape (Din,).\n",
        "    \n",
        "    Inputs:\n",
        "    - x: A numpy array containing input data, of shape (N, Din)\n",
        "    - w: A numpy array of weights, of shape (Din, Dout)\n",
        "    - b: A numpy array of biases, of shape (Dout,)\n",
        "    \n",
        "    Returns a tuple of:\n",
        "    - out: output, of shape (N, Dout)\n",
        "    - cache: (x, w, b)\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the forward pass. Store the result in out.              #\n",
        "    ###########################################################################\n",
        "    \n",
        "    out = np.matmul(x, w) + b   # numpy array broadcasts the bias\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w, b)\n",
        "    \n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def fc_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a fully_connected layer.\n",
        "    \n",
        "    Inputs:\n",
        "    - dout: Upstream derivative, of shape (N, Dout)\n",
        "    - cache: returned by your forward function. Tuple of:\n",
        "      - x: Input data, of shape (N, Din)\n",
        "      - w: Weights, of shape (Din, Dout)\n",
        "      - b: Biases, of shape (Dout,)\n",
        "      \n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x, of shape (N, Din)\n",
        "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
        "    - db: Gradient with respect to b, of shape (Dout,)\n",
        "    \"\"\"\n",
        "    x, w, b = cache\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the affine backward pass.                               #\n",
        "    ###########################################################################   \n",
        "    \n",
        "    dx = np.matmul(dout, np.transpose(w))\n",
        "    dw = np.matmul(np.transpose(x), dout)\n",
        "    db = np.matmul(np.transpose(dout), np.ones((x.shape[0], )))\n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw, db\n",
        "\n",
        "def relu_forward(x):\n",
        "    \"\"\"\n",
        "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - x: Inputs, of any shape\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - out: Output, of the same shape as x\n",
        "    - cache: x\n",
        "    \"\"\"\n",
        "    \n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU forward pass.                                  #\n",
        "    ###########################################################################\n",
        "    \n",
        "    out = np.where(x >= 0, x, 0)    \n",
        "    \n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = x\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
        "\n",
        "    Input:\n",
        "    - dout: Upstream derivatives, of any shape\n",
        "    - cache: returned by your forward function. Input x, of same shape as dout\n",
        "\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "     \n",
        "\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the ReLU backward pass.                                 #\n",
        "    ###########################################################################\n",
        "\n",
        "    dx = np.where(cache >= 0, dout, 0)\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n",
        "\n",
        "def softmax_loss(x, y):\n",
        "    \"\"\"\n",
        "    Computes the loss and gradient for softmax classification.\n",
        "    The loss that we are using is the cross entropy loss. \n",
        "    \n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
        "      class for the ith input.\n",
        "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
        "      0 <= y[i] < C\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss: Scalar giving the loss\n",
        "    - dx: Gradient of the loss with respect to x\n",
        "    \"\"\"\n",
        "    ###########################################################################\n",
        "    # TODO: Implement softmax loss                                            #\n",
        "    ###########################################################################\n",
        "    \n",
        "    loss = 0\n",
        "    N = x.shape[0]\n",
        "    C = x.shape[1]\n",
        "\n",
        "    # subtract the max score in each example  \n",
        "    x -= np.tile(x.max(axis=1), (C,1)).transpose()\n",
        "    \n",
        "    # calculate the loss and gradients\n",
        "    loss = -np.log(np.exp(x[range(N),y]) / np.sum(np.exp(x), axis=1))\n",
        "    loss = loss.sum() / N\n",
        "\n",
        "    dx = np.exp(x) / np.tile(np.sum(np.exp(x), axis=1), (C,1)).transpose()\n",
        "    dx[range(N),y] -= 1\n",
        "    dx = dx / N\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return loss, dx"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm_2JxQkc6jU"
      },
      "source": [
        "# 2.2 Softmax Classifier\n",
        "\n",
        "In this problem, implement softmax classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQVqKQ3c3TM"
      },
      "source": [
        "class SoftmaxClassifier(object):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network with\n",
        "    softmax loss that uses a modular layer design. We assume an input dimension\n",
        "    of D, a hidden dimension of H, and perform classification over C classes.\n",
        "\n",
        "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
        "\n",
        "    The learnable parameters of the model are stored in the dictionary\n",
        "    self.params that maps parameter names to numpy arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
        "                 weight_scale=1e-3):\n",
        "        \"\"\"\n",
        "        Initialize a new network.\n",
        "\n",
        "        Inputs:\n",
        "        - input_dim: An integer giving the size of the input\n",
        "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
        "          if there's no hidden layer.\n",
        "        - num_classes: An integer giving the number of classes to classify\n",
        "        - weight_scale: Scalar giving the standard deviation for random\n",
        "          initialization of the weights.\n",
        "        \"\"\"\n",
        "        self.params = {}\n",
        "        self.hidden_dim = hidden_dim\n",
        "        ############################################################################\n",
        "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
        "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
        "        # standard deviation equal to weight_scale, and biases should be           #\n",
        "        # initialized to zero. All weights and biases should be stored in the      #\n",
        "        # dictionary self.params, with fc weights and biases using the keys        #\n",
        "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
        "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
        "        # Hint: np.random.normal\n",
        "        ############################################################################\n",
        "\n",
        "        self.params = {\n",
        "            \"W1\": np.random.normal(0, weight_scale, (input_dim, hidden_dim)),\n",
        "            \"b1\": np.zeros((hidden_dim,)),\n",
        "            \"W2\": np.random.normal(0, weight_scale, (hidden_dim, num_classes)),\n",
        "            \"b2\": np.zeros((num_classes,))\n",
        "        }\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "\n",
        "    def forwards_backwards(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Compute loss and gradient for a minibatch of data.\n",
        "\n",
        "        Inputs:\n",
        "        - X: Array of input data of shape (N, Din)\n",
        "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
        "\n",
        "        Returns:\n",
        "        If y is None, then run a test-time forward pass of the model and return:\n",
        "        - scores: Array of shape (N, C) giving classification scores, where\n",
        "          scores[i, c] is the classification score for X[i] and class c.\n",
        "\n",
        "        If y is not None, then run a training-time forward and backward pass. And\n",
        "        return a tuple of:\n",
        "        - loss: Scalar value giving the loss\n",
        "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
        "          names to gradients of the loss with respect to those parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
        "        # class scores for X and storing them in the scores variable.              #\n",
        "        ############################################################################\n",
        "\n",
        "        W1, b1 = self.params['W1'], self.params['b1']\n",
        "        W2, b2 = self.params['W2'], self.params['b2']\n",
        "\n",
        "        # fc\n",
        "        hidden, cache_hidden = fc_forward(X, W1, b1)\n",
        "        # relu        \n",
        "        hidden_relu, cache_relu = relu_forward(hidden)\n",
        "        # fc\n",
        "        scores, cache_scores = fc_forward(hidden_relu, W2, b2)\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "\n",
        "        # If y is None then we are in test mode so just return scores\n",
        "        if y is None:\n",
        "            return scores\n",
        "\n",
        "        loss, grads = 0, {}\n",
        "        ############################################################################\n",
        "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
        "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
        "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
        "        # self.params[k].                                                          # \n",
        "        ############################################################################\n",
        "\n",
        "        # loss\n",
        "        loss, dx = softmax_loss(scores, y)\n",
        "        # fc_back        \n",
        "        dx_scores, dw_scores, db_scores = fc_backward(dx, cache_scores)\n",
        "        # relu_back\n",
        "        dx_relu = relu_backward(dx_scores, cache_relu)\n",
        "        # fc_back\n",
        "        _, dw_hidden, db_hidden = fc_backward(dx_relu, cache_hidden)       \n",
        "        \n",
        "        grads = {\n",
        "            \"W1\": dw_hidden,\n",
        "            \"b1\": db_hidden,\n",
        "            \"W2\": dw_scores,\n",
        "            \"b2\": db_scores\n",
        "        }\n",
        "\n",
        "        ############################################################################\n",
        "        #                             END OF YOUR CODE                             #\n",
        "        ############################################################################\n",
        "        return loss, grads\n",
        "\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_s2l1muddC8"
      },
      "source": [
        "# 2.3 Training\n",
        "\n",
        "In this problem, you will train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSdzycBkdddp",
        "outputId": "f86eac6b-aada-43d1-b478-5f86f1b13f71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding=\"latin1\")\n",
        "    return dict\n",
        "\n",
        "def load_cifar10():\n",
        "    data = {}\n",
        "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
        "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
        "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
        "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
        "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
        "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
        "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
        "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
        "                         batch4['data'], batch5['data']))\n",
        "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
        "                       batch4['labels'] + batch5['labels'])\n",
        "    X_test = test_batch['data']\n",
        "    Y_test = test_batch['labels']\n",
        "    \n",
        "    #Preprocess images here                                     \n",
        "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
        "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
        "\n",
        "    data['X_train'] = X_train[:40000]\n",
        "    data['y_train'] = Y_train[:40000]\n",
        "    data['X_val'] = X_train[40000:]\n",
        "    data['y_val'] = Y_train[40000:]\n",
        "    data['X_test'] = X_test\n",
        "    data['y_test'] = Y_test\n",
        "    return data\n",
        "\n",
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    \n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        \n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "        \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "data = load_cifar10() \n",
        "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
        "                                    'X_val', 'y_val']}\n",
        "\n",
        "\n",
        "# initialize model\n",
        "model = SoftmaxClassifier(hidden_dim =250, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.1,\n",
        "    lr_decay=.92, num_epochs=10, \n",
        "    batch_size=150, print_every=1000)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 2660) loss: 2.308478\n",
            "(Epoch 0 / 10) train acc: 0.144000; val_acc: 0.140800\n",
            "(Epoch 1 / 10) train acc: 0.469000; val_acc: 0.437200\n",
            "(Epoch 2 / 10) train acc: 0.529000; val_acc: 0.473800\n",
            "(Epoch 3 / 10) train acc: 0.574000; val_acc: 0.456900\n",
            "(Iteration 1001 / 2660) loss: 1.432295\n",
            "(Epoch 4 / 10) train acc: 0.594000; val_acc: 0.494000\n",
            "(Epoch 5 / 10) train acc: 0.594000; val_acc: 0.497900\n",
            "(Epoch 6 / 10) train acc: 0.632000; val_acc: 0.497300\n",
            "(Epoch 7 / 10) train acc: 0.684000; val_acc: 0.503900\n",
            "(Iteration 2001 / 2660) loss: 0.838271\n",
            "(Epoch 8 / 10) train acc: 0.712000; val_acc: 0.502000\n",
            "(Epoch 9 / 10) train acc: 0.714000; val_acc: 0.514200\n",
            "(Epoch 10 / 10) train acc: 0.763000; val_acc: 0.512700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IPSfJ9SeiP9"
      },
      "source": [
        "# Report Accuracy\n",
        "\n",
        "Run the given code and report the accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZCg1YqKUhOH"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqErweYekZj",
        "outputId": "a35b748d-caaa-422b-b4cd-47cb29d52187",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# report test accuracy\n",
        "acc = test_network(model, data['X_test'], data['y_test'])\n",
        "print(\"Test accuracy: {}\".format(acc))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.5116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61QIRkyVepr2"
      },
      "source": [
        "# 2.4 Plot\n",
        "\n",
        "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJTasbZevsO",
        "outputId": "62fa2ac8-7403-4ed7-dbca-38dda573b6fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(train_acc_history, label = \"Train accuracy\")\n",
        "plt.plot(val_acc_history, label = \"Validation accuracy\")\n",
        "plt.xlabel(\"Number of epochs\")\n",
        "plt.ylabel(\"% accuracy\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc1iZEqKeyK3"
      },
      "source": [
        "# Problem 3: Convolutional Neural Network\n",
        "In this problem you will train a network with the following architecture: conv - relu - 2x2 max pool - fc - relu - fc - softmax, and test it out on the MNIST dataset.\n",
        "\n",
        "The outputs of the last fully connected layer are the scores for each class.\n",
        "\n",
        "You cannot use any deep learning libraries such as PyTorch in this part.\n",
        "\n",
        "GREAT RESOURCE: https://cs231n.github.io/convolutional-networks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJTy3spfTjA"
      },
      "source": [
        "# Layers\n",
        "In this part, we implement the layers you need for your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8mlTosufPEV"
      },
      "source": [
        "def conv_forward(x, w):\n",
        "    \"\"\"\n",
        "    The input consists of N data points, each with C channels, height H and\n",
        "    width W. We filter each input with F different filters, where each filter\n",
        "    spans all C channels and has height HH and width WW. \n",
        "    Input:\n",
        "    - x: Input data of shape (N, C, H, W)\n",
        "    - w: Filter weights of shape (F, C, HH, WW)\n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
        "      H' = H - HH + 1\n",
        "      W' = W - WW + 1\n",
        "    - cache: (x, w)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional forward pass.                         #\n",
        "    ###########################################################################\n",
        "    # Extract shapes and constants\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    H_out = H - HH + 1\n",
        "    W_out = W - WW + 1\n",
        "\n",
        "    out = []\n",
        "\n",
        "    # Compute 2d convolution of the input data and the weight   \n",
        "    for input_data in x:\n",
        "      out_ = np.zeros((F, H_out, W_out))\n",
        "\n",
        "      # stride the weight over the receptive fields (stride: 1)\n",
        "      for row in range(H_out):\n",
        "        for col in range(W_out):\n",
        "          # multiply the input data (C, H', W') and the weight (F, C, H', W')\n",
        "          # and accumulate over (C, H', W') axes\n",
        "          out_[:, row, col] = (input_data[:, row:row+HH, \\\n",
        "                                          col:col+HH] * w).sum(axis=(1,2,3))   \n",
        "\n",
        "      out.append(out_)\n",
        "\n",
        "    out = np.array(out)\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, w)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def conv_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives.\n",
        "    - cache: A tuple of (x, w) as in conv_forward\n",
        "    Returns a tuple of:\n",
        "    - dx: Gradient with respect to x\n",
        "    - dw: Gradient with respect to w\n",
        "    \"\"\"\n",
        "    dx, dw = None, None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the convolutional backward pass.                        #\n",
        "    ###########################################################################\n",
        "\n",
        "    x, w = cache\n",
        "    N, C, H, W = x.shape\n",
        "    F, _, HH, WW = w.shape\n",
        "\n",
        "    H_out = H - HH + 1\n",
        "    W_out = W - WW + 1\n",
        "\n",
        "    dx = np.zeros_like(x)\n",
        "    dw = np.zeros_like(w)\n",
        "\n",
        "    # zero-pad dout for dx convolution\n",
        "    dout_pad = np.pad(dout, ((0,0),(0,0),(HH-1,HH-1),(WW-1,WW-1)), 'constant')    \n",
        "\n",
        "    # dx (unused in this architecture): looping over channel (C) dimensions\n",
        "    for ind_ch in range(C):\n",
        "      # looping over x-y coordinates\n",
        "      for row in range(H):\n",
        "        for col in range(W):\n",
        "          dx[:, ind_ch, row, col] = (dout_pad[:, :, row:row+HH, col:col+WW] * \\\n",
        "                                     w[:, ind_ch, ::-1, ::-1]).sum()\n",
        "\n",
        "    # dw: looping over input(C)/output(F) channel dimensions\n",
        "    for ind_filter in range(F):\n",
        "      for ind_ch in range(C):\n",
        "        # looping over x-y coordinates\n",
        "        for row in range(HH):\n",
        "          for col in range(WW):         \n",
        "            dw[ind_filter, ind_ch, row, col] = \\\n",
        "            (dout[:, ind_filter, :, :] * x[:, ind_ch, row:row+H_out, col:col+W_out]).sum() / N\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx, dw\n",
        "  \n",
        "\n",
        "def max_pool_forward(x, pool_param):\n",
        "    \"\"\"\n",
        "    A naive implementation of the forward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - x: Input data, of shape (N, C, H, W)\n",
        "    - pool_param: dictionary with the following keys:\n",
        "      - 'pool_height': The height of each pooling region\n",
        "      - 'pool_width': The width of each pooling region\n",
        "      - 'stride': The distance between adjacent pooling regions\n",
        "    No padding is necessary here. Output size is given by \n",
        "    Returns a tuple of:\n",
        "    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n",
        "      H' = 1 + (H - pool_height) / stride\n",
        "      W' = 1 + (W - pool_width) / stride\n",
        "    - cache: (x, pool_param)\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling forward pass                            #\n",
        "    ###########################################################################\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    pool_height = pool_param[\"pool_height\"]\n",
        "    pool_width = pool_param[\"pool_width\"]\n",
        "    stride = pool_param[\"stride\"]\n",
        "\n",
        "    # As the problem states, assume that the pooling parameters are given \n",
        "    # so as to satisfy the right fit to the input data without the padding\n",
        "\n",
        "    H_out = 1 + (H - pool_height) // stride   # '/' is okay too for the given params\n",
        "    W_out = 1 + (W - pool_width) // stride\n",
        "\n",
        "    out = np.zeros((N, C, H_out, W_out))\n",
        "\n",
        "    for row in range(H_out):\n",
        "      for col in range(W_out):\n",
        "        out[:, :, row, col] = x[:, :, row*stride:row*stride+pool_height, \\\n",
        "                                col*stride:col*stride+pool_width].max(axis=(2,3))\n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    cache = (x, pool_param)\n",
        "    return out, cache\n",
        "\n",
        "\n",
        "def max_pool_backward(dout, cache):\n",
        "    \"\"\"\n",
        "    A naive implementation of the backward pass for a max-pooling layer.\n",
        "    Inputs:\n",
        "    - dout: Upstream derivatives\n",
        "    - cache: A tuple of (x, pool_param) as in the forward pass.\n",
        "    Returns:\n",
        "    - dx: Gradient with respect to x\n",
        "    \"\"\"\n",
        "    dx = None\n",
        "    ###########################################################################\n",
        "    # TODO: Implement the max-pooling backward pass                           #\n",
        "    ###########################################################################\n",
        "    x, pool_param = cache\n",
        "\n",
        "    N, C, H, W = x.shape\n",
        "\n",
        "    pool_height = pool_param[\"pool_height\"]\n",
        "    pool_width = pool_param[\"pool_width\"]\n",
        "    stride = pool_param[\"stride\"]\n",
        "\n",
        "    H_out = 1 + (H - pool_height) // stride   # '/' is okay too for the given params\n",
        "    W_out = 1 + (W - pool_width) // stride\n",
        "\n",
        "    dx = np.zeros((N, C, H, W))\n",
        "\n",
        "    # looping over N and C\n",
        "    for ind_ex in range(N):\n",
        "      for ind_ch in range(C):\n",
        "        # looping over x-y coordinates\n",
        "        for row in range(H_out):\n",
        "          for col in range(W_out):\n",
        "            x_pool = x[ind_ex, ind_ch, row*stride:row*stride+pool_height, \\\n",
        "                       col*stride:col*stride+pool_width]\n",
        "            \n",
        "            # retrieve the coordinate with the maximum value within the subwindow (2x2)\n",
        "            row_max, col_max = np.unravel_index(np.argmax(x_pool), x_pool.shape)\n",
        "            dx[ind_ex, ind_ch, row_max, col_max] = dout[ind_ex, ind_ch, row_max, col_max]      \n",
        "\n",
        "    ###########################################################################\n",
        "    #                             END OF YOUR CODE                            #\n",
        "    ###########################################################################\n",
        "    return dx\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDZy7Mw6fiOx"
      },
      "source": [
        "# 3.1 Classifier with convolutional layer\n",
        "\n",
        "In this problem, implement ConvNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANN0nOfYfhPN"
      },
      "source": [
        "class ConvNet(object):\n",
        "  \"\"\"\n",
        "  A convolutional network with the following architecture:\n",
        "  \n",
        "  conv - relu - 2x2 max pool - fc - relu - fc - softmax\n",
        "  You may also consider adding dropout layer or batch normalization layer. \n",
        "  \n",
        "  The network operates on minibatches of data that have shape (N, C, H, W)\n",
        "  consisting of N images, each with height H and width W and with C input\n",
        "  channels.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, input_dim=(1, 28, 28), num_filters=32, filter_size=7,\n",
        "               hidden_dim=100, num_classes=10, weight_scale=1e-3, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    Initialize a new network.\n",
        "    \n",
        "    Inputs:\n",
        "    - input_dim: Tuple (C, H, W) giving size of input data\n",
        "    - num_filters: Number of filters to use in the convolutional layer\n",
        "    - filter_size: Size of filters to use in the convolutional layer\n",
        "    - hidden_dim: Number of units to use in the fully-connected hidden layer\n",
        "    - num_classes: Number of scores to produce from the final affine layer.\n",
        "    - weight_scale: Scalar giving standard deviation for random initialization\n",
        "      of weights.\n",
        "    - dtype: numpy datatype to use for computation.\n",
        "    \"\"\"\n",
        "    self.params = {}\n",
        "    self.dtype = dtype\n",
        "    \n",
        "    ############################################################################\n",
        "    # TODO: Initialize weights and biases for the three-layer convolutional    #\n",
        "    # network. Weights should be initialized from a Gaussian with standard     #\n",
        "    # deviation equal to weight_scale; biases should be initialized to zero.   #\n",
        "    # All weights and biases should be stored in the dictionary self.params.   #\n",
        "    # Store weights for the convolutional layer using the keys 'W1' (here      #\n",
        "    # we do not consider the bias term in the convolutional layer);            #\n",
        "    # use keys 'W2' and 'b2' for the weights and biases of the                 #\n",
        "    # hidden fully-connected layer, and keys 'W3' and 'b3' for the weights     #\n",
        "    # and biases of the output affine layer.                                   #\n",
        "    ############################################################################\n",
        "\n",
        "    H_convout = input_dim[1] - filter_size + 1\n",
        "    W_convout = input_dim[2] - filter_size + 1\n",
        "\n",
        "    self.params = {\n",
        "      \"W1\": np.random.normal(0, weight_scale, (num_filters, input_dim[0], filter_size, filter_size)),\n",
        "      \"W2\": np.random.normal(0, weight_scale, (num_filters*H_convout*W_convout//4, hidden_dim)),\n",
        "      \"b2\": np.zeros((hidden_dim,)),\n",
        "      \"W3\": np.random.normal(0, weight_scale, (hidden_dim, num_classes)),\n",
        "      \"b3\": np.zeros((num_classes,))\n",
        "    }\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "\n",
        "    for k, v in self.params.items():\n",
        "      self.params[k] = v.astype(dtype)\n",
        "     \n",
        " \n",
        "  def forwards_backwards(self, X, y=None):\n",
        "    \"\"\"\n",
        "    Evaluate loss and gradient for the three-layer convolutional network.\n",
        "    \n",
        "    Input / output: Same API as TwoLayerNet in fc_net.py.\n",
        "    \"\"\"\n",
        "\n",
        "    W1 = self.params['W1']\n",
        "    W2, b2 = self.params['W2'], self.params['b2']\n",
        "    W3, b3 = self.params['W3'], self.params['b3']\n",
        "    \n",
        "    # pass pool_param to the forward pass for the max-pooling layer\n",
        "    pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
        "\n",
        "    scores = None\n",
        "    ############################################################################\n",
        "    # TODO: Implement the forward pass for the three-layer convolutional net,  #\n",
        "    # computing the class scores for X and storing them in the scores          #\n",
        "    # variable.                                                                #\n",
        "    ############################################################################\n",
        "\n",
        "    N = X.shape[0]\n",
        "\n",
        "    # conv\n",
        "    convout, cache_convout = conv_forward(X, W1)\n",
        "    # relu\n",
        "    relu_convout, cache_relu_convout = relu_forward(convout)\n",
        "    # 2x2 max pool\n",
        "    poolout, cache_poolout = max_pool_forward(relu_convout, pool_param)\n",
        "    \n",
        "    # flatten poolout to feed the fully-connected layer\n",
        "    poolout_flat = poolout.reshape((N,-1))\n",
        "    \n",
        "    # fc\n",
        "    hidden, cache_hidden = fc_forward(poolout_flat, W2, b2)\n",
        "    # relu\n",
        "    relu_hidden, cache_relu_hidden = relu_forward(hidden)\n",
        "    # fc\n",
        "    scores, cache_scores = fc_forward(relu_hidden, W3, b3)\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    if y is None:\n",
        "      return scores\n",
        "    \n",
        "    loss, grads = 0, {}\n",
        "    ############################################################################\n",
        "    # TODO: Implement the backward pass for the three-layer convolutional net, #\n",
        "    # storing the loss and gradients in the loss and grads variables. Compute  #\n",
        "    # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
        "    # for self.params[k].                                                      #\n",
        "    ############################################################################\n",
        "\n",
        "    # softmax loss\n",
        "    loss, dx = softmax_loss(scores, y)\n",
        "    # fc_score_back\n",
        "    dx_scores, dw_scores, db_scores = fc_backward(dx, cache_scores)\n",
        "    # relu_hidden_back    \n",
        "    dx_relu_hidden = relu_backward(dx_scores, cache_relu_hidden)\n",
        "    # fc_hidden_back\n",
        "    dx_hidden, dw_hidden, db_hidden = fc_backward(dx_relu_hidden, cache_hidden)\n",
        "    \n",
        "    # reshape dx_hidden to feed max_pool_backward\n",
        "    dx_hidden = dx_hidden.reshape(poolout.shape)\n",
        "\n",
        "    # pool_back\n",
        "    dx_poolout = max_pool_backward(dx_hidden, cache_poolout)\n",
        "    # relu_convout_back    \n",
        "    dx_relu_convout = relu_backward(dx_poolout, cache_relu_convout)\n",
        "\n",
        "    # conv_back\n",
        "    _, dw_conv = conv_backward(dx_relu_convout, cache_convout)\n",
        "\n",
        "    grads = {\n",
        "        \"W1\": dw_conv,\n",
        "        \"W2\": dw_hidden,\n",
        "        \"b2\": db_hidden,\n",
        "        \"W3\": dw_scores,\n",
        "        \"b3\": db_scores\n",
        "    }\n",
        "\n",
        "    ############################################################################\n",
        "    #                             END OF YOUR CODE                             #\n",
        "    ############################################################################\n",
        "    \n",
        "    return loss, grads"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxIEeJ_BgEA-"
      },
      "source": [
        "# 3.2 Training\n",
        "\n",
        "In this problem, you need to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou1mepm_gghC",
        "outputId": "1e3ec1f6-6682-49c6-fdbe-554373686b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
        "    \"\"\"\n",
        "    Check accuracy of the model on the provided data.\n",
        "\n",
        "    Inputs:\n",
        "    - model: Image classifier\n",
        "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
        "    - y: Array of labels, of shape (N,)\n",
        "    - num_samples: If not None, subsample the data and only test the model\n",
        "      on num_samples datapoints.\n",
        "    - batch_size: Split X and y into batches of this size to avoid using\n",
        "      too much memory.\n",
        "\n",
        "    Returns:\n",
        "    - acc: Scalar giving the fraction of instances that were correctly\n",
        "      classified by the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Subsample the data\n",
        "    N = X.shape[0]\n",
        "    if num_samples is not None and N > num_samples:\n",
        "        mask = np.random.choice(N, num_samples)\n",
        "        N = num_samples\n",
        "        X = X[mask]\n",
        "        y = y[mask]\n",
        "\n",
        "    # Compute predictions in batches\n",
        "    num_batches = N // batch_size\n",
        "    if N % batch_size != 0:\n",
        "        num_batches += 1\n",
        "    y_pred = []\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        scores = model.forwards_backwards(X[start:end])\n",
        "        y_pred.append(np.argmax(scores, axis=1))\n",
        "    y_pred = np.hstack(y_pred)\n",
        "    acc = np.mean(y_pred == y)\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train_network(model, data, **kwargs):\n",
        "    \"\"\"\n",
        "     Required arguments:\n",
        "    - model: Image classifier\n",
        "    - data: A dictionary of training and validation data containing:\n",
        "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
        "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
        "      'y_train': Array, shape (N_train,) of labels for training images\n",
        "      'y_val': Array, shape (N_val,) of labels for validation images\n",
        "\n",
        "    Optional arguments:\n",
        "    - learning_rate: A scalar for initial learning rate.\n",
        "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
        "      learning rate is multiplied by this value.\n",
        "    - batch_size: Size of minibatches used to compute loss and gradient\n",
        "      during training.\n",
        "    - num_epochs: The number of epochs to run for during training.\n",
        "    - print_every: Integer; training losses will be printed every\n",
        "      print_every iterations.\n",
        "    - verbose: Boolean; if set to false then no output will be printed\n",
        "      during training.\n",
        "    - num_train_samples: Number of training samples used to check training\n",
        "      accuracy; default is 1000; set to None to use entire training set.\n",
        "    - num_val_samples: Number of validation samples to use to check val\n",
        "      accuracy; default is None, which uses the entire validation set.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
        "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
        "    batch_size = kwargs.pop('batch_size', 100)\n",
        "    num_epochs = kwargs.pop('num_epochs', 10)\n",
        "    num_train_samples = kwargs.pop('num_train_samples', 200)\n",
        "    num_val_samples = kwargs.pop('num_val_samples', 200)\n",
        "    print_every = kwargs.pop('print_every', 10)   \n",
        "    verbose = kwargs.pop('verbose', True)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_val_acc = 0\n",
        "    best_params = {}\n",
        "    loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "    \n",
        "    \n",
        "    num_train = data['X_train'].shape[0]\n",
        "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
        "    num_iterations = num_epochs * iterations_per_epoch\n",
        "    num_iterations = 60\n",
        "\n",
        "    \n",
        "    for t in range(num_iterations):\n",
        "        # Make a minibatch of training data\n",
        "        batch_mask = np.random.choice(num_train, batch_size)\n",
        "        X_batch = data['X_train'][batch_mask]\n",
        "        y_batch = data['y_train'][batch_mask]\n",
        "        # Compute loss and gradient\n",
        "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Perform a parameter update\n",
        "        for p, w in model.params.items():\n",
        "            model.params[p] = w - grads[p]*learning_rate\n",
        "          \n",
        "        # Print training loss\n",
        "        if verbose and t % print_every == 0:\n",
        "            print('(Iteration %d / %d) loss: %f' % (\n",
        "                   t + 1, num_iterations, loss_history[-1]))\n",
        "         \n",
        "        # At the end of every epoch, increment the epoch counter and decay\n",
        "        # the learning rate.\n",
        "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
        "        if epoch_end:\n",
        "            epoch += 1\n",
        "            learning_rate *= lr_decay\n",
        "        \n",
        "        \n",
        "        # Check train and val accuracy on the first iteration, the last\n",
        "        # iteration, and at the end of each epoch.\n",
        "        first_it = (t == 0)\n",
        "        last_it = (t == num_iterations - 1)\n",
        "        if first_it or last_it or epoch_end:\n",
        "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
        "                num_samples= num_train_samples)\n",
        "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
        "                num_samples=num_val_samples)\n",
        "            train_acc_history.append(train_acc)\n",
        "            val_acc_history.append(val_acc)\n",
        "           \n",
        "\n",
        "            if verbose:\n",
        "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
        "                       epoch, num_epochs, train_acc, val_acc))\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_params = {}\n",
        "                for k, v in model.params.items():\n",
        "                    best_params[k] = v.copy()\n",
        "   \n",
        "    model.params = best_params\n",
        "        \n",
        "    return model, train_acc_history, val_acc_history\n",
        "        \n",
        "\n",
        "# load data\n",
        "# data = load_cifar10() \n",
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "train_data = { 'X_train':x_train[:,None,:,:], 'X_val': x_test[:,None,:,:], 'y_train': y_train, 'y_val': y_test}\n",
        "\n",
        "\n",
        "\n",
        "# initialize model\n",
        "model = ConvNet(hidden_dim =250, weight_scale=1e-2)\n",
        "\n",
        "# start training    \n",
        "model, train_acc_history, val_acc_history = train_network(\n",
        "    model, train_data, learning_rate = 0.001,\n",
        "    lr_decay=.95, num_epochs=1, \n",
        "    batch_size=32, print_every=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(Iteration 1 / 60) loss: 2.259490\n",
            "(Epoch 0 / 1) train acc: 0.155000; val_acc: 0.140000\n",
            "(Iteration 2 / 60) loss: 2.346610\n",
            "(Iteration 3 / 60) loss: 2.454739\n",
            "(Iteration 4 / 60) loss: 2.407785\n",
            "(Iteration 5 / 60) loss: 2.265858\n",
            "(Iteration 6 / 60) loss: 2.167770\n",
            "(Iteration 7 / 60) loss: 2.123907\n",
            "(Iteration 8 / 60) loss: 2.198617\n",
            "(Iteration 9 / 60) loss: 2.171868\n",
            "(Iteration 10 / 60) loss: 2.074297\n",
            "(Iteration 11 / 60) loss: 2.112870\n",
            "(Iteration 12 / 60) loss: 2.032037\n",
            "(Iteration 13 / 60) loss: 1.977449\n",
            "(Iteration 14 / 60) loss: 2.144631\n",
            "(Iteration 15 / 60) loss: 1.982128\n",
            "(Iteration 16 / 60) loss: 2.040505\n",
            "(Iteration 17 / 60) loss: 2.019853\n",
            "(Iteration 18 / 60) loss: 1.987893\n",
            "(Iteration 19 / 60) loss: 1.887530\n",
            "(Iteration 20 / 60) loss: 1.750572\n",
            "(Iteration 21 / 60) loss: 1.977837\n",
            "(Iteration 22 / 60) loss: 1.804716\n",
            "(Iteration 23 / 60) loss: 1.805672\n",
            "(Iteration 24 / 60) loss: 1.835275\n",
            "(Iteration 25 / 60) loss: 1.823687\n",
            "(Iteration 26 / 60) loss: 1.824281\n",
            "(Iteration 27 / 60) loss: 1.653487\n",
            "(Iteration 28 / 60) loss: 1.712328\n",
            "(Iteration 29 / 60) loss: 1.743180\n",
            "(Iteration 30 / 60) loss: 1.563610\n",
            "(Iteration 31 / 60) loss: 1.580502\n",
            "(Iteration 32 / 60) loss: 1.585671\n",
            "(Iteration 33 / 60) loss: 1.666046\n",
            "(Iteration 34 / 60) loss: 1.425811\n",
            "(Iteration 35 / 60) loss: 1.598453\n",
            "(Iteration 36 / 60) loss: 1.532332\n",
            "(Iteration 37 / 60) loss: 1.484371\n",
            "(Iteration 38 / 60) loss: 1.324854\n",
            "(Iteration 39 / 60) loss: 1.572691\n",
            "(Iteration 40 / 60) loss: 1.468534\n",
            "(Iteration 41 / 60) loss: 1.486558\n",
            "(Iteration 42 / 60) loss: 1.488612\n",
            "(Iteration 43 / 60) loss: 1.461236\n",
            "(Iteration 44 / 60) loss: 1.233243\n",
            "(Iteration 45 / 60) loss: 1.402980\n",
            "(Iteration 46 / 60) loss: 1.393759\n",
            "(Iteration 47 / 60) loss: 1.382231\n",
            "(Iteration 48 / 60) loss: 1.396200\n",
            "(Iteration 49 / 60) loss: 1.267750\n",
            "(Iteration 50 / 60) loss: 1.195059\n",
            "(Iteration 51 / 60) loss: 1.321679\n",
            "(Iteration 52 / 60) loss: 1.121220\n",
            "(Iteration 53 / 60) loss: 1.083189\n",
            "(Iteration 54 / 60) loss: 1.263796\n",
            "(Iteration 55 / 60) loss: 1.150269\n",
            "(Iteration 56 / 60) loss: 1.138182\n",
            "(Iteration 57 / 60) loss: 1.385869\n",
            "(Iteration 58 / 60) loss: 1.223154\n",
            "(Iteration 59 / 60) loss: 1.033180\n",
            "(Iteration 60 / 60) loss: 1.070849\n",
            "(Epoch 0 / 1) train acc: 0.740000; val_acc: 0.800000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}